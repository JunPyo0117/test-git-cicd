name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: ap-northeast-2
  ECR_REPOSITORY_NAME: cicd-demo
  EKS_CLUSTER_NAME: cicd-cluster
  # ALB가 퍼블릭 서브넷에 위치함을 명시
  ALB_SCHEME: internet-facing

permissions:
  id-token: write
  contents: read

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install Frontend Dependencies
      working-directory: ./frontend
      run: npm ci
    
    - name: Install Backend Dependencies
      working-directory: ./backend
      run: npm ci

  build-and-deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    # AWS 자격 증명 구성 (OIDC 방식)
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::471303021447:role/github-actions-role
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: github-actions-${{ github.run_id }}
        role-duration-seconds: 3600
    
    # ECR 로그인
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # 백엔드 빌드 및 푸시
    - name: Build and push Backend image
      working-directory: ./backend
      run: |
        npm ci
        npm run build
        docker build -t backend .
        docker tag backend:latest ${{ steps.login-ecr.outputs.registry }}/cicd-demo-backend:latest
        docker push ${{ steps.login-ecr.outputs.registry }}/cicd-demo-backend:latest
    
    # 프론트엔드 빌드 및 푸시
    - name: Build and push Frontend image
      working-directory: ./frontend
      env:
        VITE_API_URL: ${{ secrets.API_URL }}
      run: |
        npm ci
        npm run build
        docker build -t frontend .
        docker tag frontend:latest ${{ steps.login-ecr.outputs.registry }}/cicd-demo-frontend:latest
        docker push ${{ steps.login-ecr.outputs.registry }}/cicd-demo-frontend:latest
    
    # EKS 설정 및 Access Entry 확인
    - name: Configure EKS access
      run: |
        aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
        aws sts get-caller-identity
        
        # Check if EKS cluster is accessible via AWS CLI
        echo "Checking EKS cluster accessibility..."
        if aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} > /dev/null 2>&1; then
          echo "EKS cluster is accessible via AWS CLI"
        else
          echo "Cannot access EKS cluster via AWS CLI"
          exit 1
        fi
        
        # Check current EKS cluster authentication mode
        echo "Checking current EKS cluster authentication mode..."
        AUTH_MODE=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --query 'cluster.accessConfig.authenticationMode' --output text)
        echo "Current authentication mode: $AUTH_MODE"
        
        # Get current AWS identity
        CURRENT_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)
        echo "Current AWS ARN: $CURRENT_ARN"
        
        # Check if EKS Access Entry exists for github-actions-role
        echo "Checking EKS Access Entry for github-actions-role..."
        if aws eks describe-access-entry --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --principal-arn "arn:aws:iam::471303021447:role/github-actions-role" > /dev/null 2>&1; then
          echo "EKS Access Entry for github-actions-role exists"
          
          # Check associated policies
          echo "Checking associated access policies..."
          POLICIES=$(aws eks list-associated-access-policies --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} --principal-arn "arn:aws:iam::471303021447:role/github-actions-role" --query 'associatedAccessPolicies[*].policyArn' --output text)
          echo "Associated policies: $POLICIES"
          
          # Check if both required policies are associated
          if echo "$POLICIES" | grep -q "AmazonEKSAdminPolicy" && echo "$POLICIES" | grep -q "AmazonEKSClusterAdminPolicy"; then
            echo "Both AmazonEKSAdminPolicy and AmazonEKSClusterAdminPolicy are associated"
          else
            echo "Missing required policies. Please ensure both AmazonEKSAdminPolicy and AmazonEKSClusterAdminPolicy are associated"
            exit 1
          fi
        else
          echo "EKS Access Entry for github-actions-role not found"
          echo "Please create EKS Access Entry with both AmazonEKSAdminPolicy and AmazonEKSClusterAdminPolicy for github-actions-role"
          exit 1
        fi
        
        # Test kubectl access
        echo "Testing kubectl access..."
        if kubectl get nodes --request-timeout=30s > /dev/null 2>&1; then
          echo "kubectl access is working!"
        else
          echo "kubectl access failed. This indicates that the EKS Access Entry is not properly configured."
          echo "Please ensure the following EKS Access Entry exists with both policies:"
          echo "1. Go to AWS Console: EKS > Clusters > cicd-cluster > Access tab"
          echo "2. Verify github-actions-role has both AmazonEKSAdminPolicy and AmazonEKSClusterAdminPolicy"
          echo "3. Or create via AWS CLI:"
          echo "   aws eks create-access-entry --cluster-name cicd-cluster --principal-arn 'arn:aws:iam::471303021447:role/github-actions-role' --type STANDARD"
          echo "   aws eks associate-access-policy --cluster-name cicd-cluster --principal-arn 'arn:aws:iam::471303021447:role/github-actions-role' --policy-arn 'arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy' --access-scope type=cluster"
          echo "   aws eks associate-access-policy --cluster-name cicd-cluster --principal-arn 'arn:aws:iam::471303021447:role/github-actions-role' --policy-arn 'arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy' --access-scope type=cluster"
          exit 1
        fi
        
        # Final test
        kubectl get nodes
    
    # AWS Load Balancer Controller ServiceAccount 생성
    - name: Create AWS Load Balancer Controller ServiceAccount
      run: |
        cat > aws-load-balancer-controller-sa.yaml << EOF
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: aws-load-balancer-controller
          namespace: kube-system
          annotations:
            eks.amazonaws.com/role-arn: arn:aws:iam::471303021447:role/aws-load-balancer-controller
        EOF
        kubectl apply -f aws-load-balancer-controller-sa.yaml --validate=false
    
    # AWS Load Balancer Controller 설치
    - name: Install AWS Load Balancer Controller
      run: |
        # Helm을 사용하여 AWS Load Balancer Controller 설치
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        # VPC ID 가져오기
        VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
        echo "VPC ID: $VPC_ID"
        
        # AWS Load Balancer Controller 설치 (기존 설치가 있다면 업그레이드)
        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          -n kube-system \
          --set clusterName=${{ env.EKS_CLUSTER_NAME }} \
          --set serviceAccount.create=false \
          --set serviceAccount.name=aws-load-balancer-controller \
          --set region=${{ env.AWS_REGION }} \
          --set vpcId=$VPC_ID \
          --wait --timeout=300s
        
        # Controller가 준비될 때까지 대기
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
    
    # 환경 변수 설정
    - name: Set environment variables
      run: |
        echo "ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}" >> $GITHUB_ENV
        echo "IMAGE_TAG=latest" >> $GITHUB_ENV
        echo "API_URL=${{ secrets.API_URL }}" >> $GITHUB_ENV
        echo "DOMAIN_NAME=${{ secrets.DOMAIN_NAME }}" >> $GITHUB_ENV
        
        # Terraform output에서 서브넷 ID와 보안 그룹 ID 가져오기
        PUBLIC_SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=tag:Name,Values=*public*" "Name=vpc-id,Values=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')
        ALB_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*alb*" "Name=vpc-id,Values=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)" --query 'SecurityGroups[0].GroupId' --output text)
        
        echo "PUBLIC_SUBNET_IDS=$PUBLIC_SUBNET_IDS" >> $GITHUB_ENV
        echo "ALB_SECURITY_GROUP_ID=$ALB_SECURITY_GROUP_ID" >> $GITHUB_ENV
    
    # 데이터베이스 시크릿 생성
    - name: Create database secret
      run: |
        # Base64 인코딩
        DB_HOST_B64=$(echo -n "${{ secrets.DB_HOST }}" | base64)
        DB_PORT_B64=$(echo -n "5432" | base64)
        DB_USERNAME_B64=$(echo -n "postgres" | base64)
        DB_PASSWORD_B64=$(echo -n "${{ secrets.DB_PASSWORD }}" | base64)
        DB_NAME_B64=$(echo -n "cicd_demo" | base64)
        
        # 시크릿 파일 생성
        cat > k8s/backend/secret-generated.yaml << EOF
        apiVersion: v1
        kind: Secret
        metadata:
          name: db-secret
          namespace: cicd-demo
        type: Opaque
        data:
          host: $DB_HOST_B64
          port: $DB_PORT_B64
          username: $DB_USERNAME_B64
          password: $DB_PASSWORD_B64
          database: $DB_NAME_B64
        EOF
    
    # Kubernetes 매니페스트 적용
    - name: Deploy to EKS
      run: |
        # 환경 변수 치환
        envsubst < k8s/backend/deployment.yaml > k8s/backend/deployment-generated.yaml
        envsubst < k8s/frontend/deployment.yaml > k8s/frontend/deployment-generated.yaml
        envsubst < k8s/ingress.yaml > k8s/ingress-generated.yaml
        envsubst < k8s/configmap.yaml > k8s/configmap-generated.yaml
        
        # 배포 (각 단계별로 확인)
        echo "Creating namespace..."
        kubectl apply -f k8s/namespace.yaml
        
        echo "Applying ConfigMap..."
        kubectl apply -f k8s/configmap-generated.yaml
        
        echo "Creating database secret..."
        kubectl apply -f k8s/backend/secret-generated.yaml
        
        echo "Deploying backend..."
        kubectl apply -f k8s/backend/deployment-generated.yaml
        kubectl apply -f k8s/backend/service.yaml
        
        echo "Deploying frontend..."
        kubectl apply -f k8s/frontend/deployment-generated.yaml
        kubectl apply -f k8s/frontend/service.yaml
        
        echo "Applying Ingress..."
        kubectl apply -f k8s/ingress-generated.yaml
        
        # 배포 상태 확인
        echo "Waiting for backend deployment to be ready..."
        kubectl rollout status deployment/backend -n cicd-demo --timeout=300s
        
        echo "Waiting for frontend deployment to be ready..."
        kubectl rollout status deployment/frontend -n cicd-demo --timeout=300s
        
        # Ingress 상태 확인 및 ALB DNS 출력
        echo "Waiting for Ingress to be ready..."
        kubectl wait --for=condition=ready ingress/cicd-ingress -n cicd-demo --timeout=300s
        
        # ALB DNS 이름 출력
        ALB_DNS=$(kubectl get ingress cicd-ingress -n cicd-demo -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        echo "ALB DNS Name: $ALB_DNS"
        echo "Backend API URL: http://$ALB_DNS"
