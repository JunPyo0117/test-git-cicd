name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: ap-northeast-2
  ECR_REPOSITORY_NAME: cicd-demo
  EKS_CLUSTER_NAME: cicd-cluster
  # ALB가 퍼블릭 서브넷에 위치함을 명시
  ALB_SCHEME: internet-facing

permissions:
  id-token: write
  contents: read

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    
    - name: Install Frontend Dependencies
      working-directory: ./frontend
      run: npm ci
    
    - name: Install Backend Dependencies
      working-directory: ./backend
      run: npm ci

  build-and-deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    # AWS 자격 증명 구성 (OIDC 방식)
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::471303021447:role/github-actions-role
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: github-actions-${{ github.run_id }}
        role-duration-seconds: 3600
    
    # ECR 로그인
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # 백엔드 빌드 및 푸시
    - name: Build and push Backend image
      working-directory: ./backend
      run: |
        npm ci
        npm run build
        docker build -t backend .
        docker tag backend:latest ${{ steps.login-ecr.outputs.registry }}/cicd-demo-backend:latest
        docker push ${{ steps.login-ecr.outputs.registry }}/cicd-demo-backend:latest
    
    # 프론트엔드 빌드 및 푸시
    - name: Build and push Frontend image
      working-directory: ./frontend
      env:
        VITE_API_URL: ${{ secrets.API_URL }}
      run: |
        npm ci
        npm run build
        docker build -t frontend .
        docker tag frontend:latest ${{ steps.login-ecr.outputs.registry }}/cicd-demo-frontend:latest
        docker push ${{ steps.login-ecr.outputs.registry }}/cicd-demo-frontend:latest
    
    # EKS 설정 및 aws-auth ConfigMap 적용
    - name: Configure EKS access
      run: |
        aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
        aws sts get-caller-identity
        
        # Apply aws-auth ConfigMap first
        echo "Applying aws-auth ConfigMap..."
        kubectl apply -f k8s/aws-auth-configmap.yaml
        
        # Wait for aws-auth ConfigMap to take effect and retry kubectl commands
        echo "Waiting for aws-auth ConfigMap to take effect..."
        for i in {1..15}; do
          if kubectl get nodes --request-timeout=30s > /dev/null 2>&1; then
            echo "Successfully connected to EKS cluster"
            break
          else
            echo "Attempt $i: Waiting for authentication to be ready..."
            if [ $i -eq 15 ]; then
              echo "Failed to connect to EKS cluster after 15 attempts"
              echo "Current AWS identity:"
              aws sts get-caller-identity
              echo "Current kubeconfig context:"
              kubectl config current-context
              echo "Available contexts:"
              kubectl config get-contexts
              echo "Trying to get cluster info using AWS CLI..."
              aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
              exit 1
            fi
            sleep 30
          fi
        done
        
        # Final test
        kubectl get nodes
    
    # AWS Load Balancer Controller ServiceAccount 생성
    - name: Create AWS Load Balancer Controller ServiceAccount
      run: |
        cat > aws-load-balancer-controller-sa.yaml << EOF
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: aws-load-balancer-controller
          namespace: kube-system
          annotations:
            eks.amazonaws.com/role-arn: arn:aws:iam::471303021447:role/aws-load-balancer-controller
        EOF
        kubectl apply -f aws-load-balancer-controller-sa.yaml
    
    # AWS Load Balancer Controller 설치
    - name: Install AWS Load Balancer Controller
      run: |
        # Helm을 사용하여 AWS Load Balancer Controller 설치
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        # VPC ID 가져오기
        VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
        echo "VPC ID: $VPC_ID"
        
        # AWS Load Balancer Controller 설치 (기존 설치가 있다면 업그레이드)
        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          -n kube-system \
          --set clusterName=${{ env.EKS_CLUSTER_NAME }} \
          --set serviceAccount.create=false \
          --set serviceAccount.name=aws-load-balancer-controller \
          --set region=${{ env.AWS_REGION }} \
          --set vpcId=$VPC_ID \
          --wait --timeout=300s
        
        # Controller가 준비될 때까지 대기
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
    
    # 환경 변수 설정
    - name: Set environment variables
      run: |
        echo "ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}" >> $GITHUB_ENV
        echo "IMAGE_TAG=latest" >> $GITHUB_ENV
        echo "API_URL=${{ secrets.API_URL }}" >> $GITHUB_ENV
        echo "DOMAIN_NAME=${{ secrets.DOMAIN_NAME }}" >> $GITHUB_ENV
        
        # Terraform output에서 서브넷 ID와 보안 그룹 ID 가져오기
        PUBLIC_SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=tag:Name,Values=*public*" "Name=vpc-id,Values=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')
        ALB_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*alb*" "Name=vpc-id,Values=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)" --query 'SecurityGroups[0].GroupId' --output text)
        
        echo "PUBLIC_SUBNET_IDS=$PUBLIC_SUBNET_IDS" >> $GITHUB_ENV
        echo "ALB_SECURITY_GROUP_ID=$ALB_SECURITY_GROUP_ID" >> $GITHUB_ENV
    
    # 데이터베이스 시크릿 생성
    - name: Create database secret
      run: |
        # Base64 인코딩
        DB_HOST_B64=$(echo -n "${{ secrets.DB_HOST }}" | base64)
        DB_PORT_B64=$(echo -n "5432" | base64)
        DB_USERNAME_B64=$(echo -n "postgres" | base64)
        DB_PASSWORD_B64=$(echo -n "${{ secrets.DB_PASSWORD }}" | base64)
        DB_NAME_B64=$(echo -n "cicd_demo" | base64)
        
        # 시크릿 파일 생성
        cat > k8s/backend/secret-generated.yaml << EOF
        apiVersion: v1
        kind: Secret
        metadata:
          name: db-secret
          namespace: cicd-demo
        type: Opaque
        data:
          host: $DB_HOST_B64
          port: $DB_PORT_B64
          username: $DB_USERNAME_B64
          password: $DB_PASSWORD_B64
          database: $DB_NAME_B64
        EOF
    
    # Kubernetes 매니페스트 적용
    - name: Deploy to EKS
      run: |
        # 환경 변수 치환
        envsubst < k8s/backend/deployment.yaml > k8s/backend/deployment-generated.yaml
        envsubst < k8s/frontend/deployment.yaml > k8s/frontend/deployment-generated.yaml
        envsubst < k8s/ingress.yaml > k8s/ingress-generated.yaml
        envsubst < k8s/configmap.yaml > k8s/configmap-generated.yaml
        
        # 배포 (각 단계별로 확인)
        echo "Creating namespace..."
        kubectl apply -f k8s/namespace.yaml
        
        echo "Applying ConfigMap..."
        kubectl apply -f k8s/configmap-generated.yaml
        
        echo "Creating database secret..."
        kubectl apply -f k8s/backend/secret-generated.yaml
        
        echo "Deploying backend..."
        kubectl apply -f k8s/backend/deployment-generated.yaml
        kubectl apply -f k8s/backend/service.yaml
        
        echo "Deploying frontend..."
        kubectl apply -f k8s/frontend/deployment-generated.yaml
        kubectl apply -f k8s/frontend/service.yaml
        
        echo "Applying Ingress..."
        kubectl apply -f k8s/ingress-generated.yaml
        
        # 배포 상태 확인
        echo "Waiting for backend deployment to be ready..."
        kubectl rollout status deployment/backend -n cicd-demo --timeout=300s
        
        echo "Waiting for frontend deployment to be ready..."
        kubectl rollout status deployment/frontend -n cicd-demo --timeout=300s
        
        # Ingress 상태 확인 및 ALB DNS 출력
        echo "Waiting for Ingress to be ready..."
        kubectl wait --for=condition=ready ingress/cicd-ingress -n cicd-demo --timeout=300s
        
        # ALB DNS 이름 출력
        ALB_DNS=$(kubectl get ingress cicd-ingress -n cicd-demo -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        echo "ALB DNS Name: $ALB_DNS"
        echo "Backend API URL: http://$ALB_DNS"
